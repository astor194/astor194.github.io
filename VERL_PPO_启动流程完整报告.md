# VERL PPO è®­ç»ƒæ¡†æ¶å¯åŠ¨æµç¨‹å®Œæ•´æŠ¥å‘Š

## ğŸ“‹ ç›®å½•
- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
- [3. å¯åŠ¨æµç¨‹è¯¦è§£](#3-å¯åŠ¨æµç¨‹è¯¦è§£)
- [4. æ ¸å¿ƒç»„ä»¶è§£æ](#4-æ ¸å¿ƒç»„ä»¶è§£æ)
- [5. PPO è®­ç»ƒå¾ªç¯](#5-ppo-è®­ç»ƒå¾ªç¯)
- [6. é…ç½®ç³»ç»Ÿ](#6-é…ç½®ç³»ç»Ÿ)
- [7. å®é™…æ¡ˆä¾‹åˆ†æ](#7-å®é™…æ¡ˆä¾‹åˆ†æ)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)
- [9. å¸¸è§é—®é¢˜](#9-å¸¸è§é—®é¢˜)

---

## 1. æ¦‚è¿°

### 1.1 VERL ç®€ä»‹

**VERL** (Volcano Engine Reinforcement Learning) æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ RLHF (Reinforcement Learning from Human Feedback) åœºæ™¯è®¾è®¡ã€‚

### 1.2 æ ¸å¿ƒç‰¹ç‚¹

- âœ… **åˆ†å¸ƒå¼æ¶æ„**: åŸºäº Ray çš„å¤šèŠ‚ç‚¹å¤š GPU å¹¶è¡Œè®­ç»ƒ
- âœ… **è§’è‰²è§£è€¦**: å°†ç”Ÿæˆã€è®­ç»ƒã€è¯„ä¼°ç­‰åŠŸèƒ½åˆ†ç¦»åˆ°ç‹¬ç«‹ workers
- âœ… **æ··åˆå¼•æ“**: æ”¯æŒç”Ÿæˆå¼•æ“ (vLLM/SGLang) ä¸è®­ç»ƒå¼•æ“ (FSDP/Megatron) åˆ†ç¦»
- âœ… **çµæ´»é…ç½®**: ä½¿ç”¨ Hydra ç®¡ç†å¤æ‚çš„è¶…å‚æ•°é…ç½®
- âœ… **å¤šæ ·ç®—æ³•**: æ”¯æŒ PPOã€GRPOã€REINFORCE++ ç­‰å¤šç§ RL ç®—æ³•

### 1.3 æŠ€æœ¯æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         åº”ç”¨å±‚ (main_ppo.py)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    é…ç½®ç®¡ç† (Hydra + OmegaConf)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    è®­ç»ƒç¼–æ’ (RayPPOTrainer)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åˆ†å¸ƒå¼æ§åˆ¶ (Ray + WorkerGroup)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ç”Ÿæˆå¼•æ“        â”‚      è®­ç»ƒå¼•æ“       â”‚
â”‚  (vLLM/SGLang)    â”‚   (FSDP/Megatron)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ·±åº¦å­¦ä¹ æ¡†æ¶ (PyTorch)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„å›¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Driver Process    â”‚
                    â”‚  (main_ppo.py)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    TaskRunner       â”‚
                    â”‚  (Ray Remote Actor) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   RayPPOTrainer     â”‚
                    â”‚   (Orchestrator)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ActorRollout   â”‚   â”‚     Critic      â”‚   â”‚   RefPolicy     â”‚
â”‚  WorkerGroup   â”‚   â”‚  WorkerGroup    â”‚   â”‚  WorkerGroup    â”‚
â”‚                â”‚   â”‚                 â”‚   â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚Worker 0 â”‚    â”‚   â”‚  â”‚Worker 0 â”‚   â”‚   â”‚  â”‚Worker 0 â”‚    â”‚
â”‚ â”‚Worker 1 â”‚    â”‚   â”‚  â”‚Worker 1 â”‚   â”‚   â”‚  â”‚Worker 1 â”‚    â”‚
â”‚ â”‚  ...    â”‚    â”‚   â”‚  â”‚  ...    â”‚   â”‚   â”‚  â”‚  ...    â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     (ç”Ÿæˆ+è®­ç»ƒ)           (ä»·å€¼ç½‘ç»œ)           (å‚è€ƒç­–ç•¥)
```

### 2.2 Worker Roles (å·¥ä½œè§’è‰²)

| Role | åŠŸèƒ½ | æ˜¯å¦å¿…éœ€ | å¤‡æ³¨ |
|------|------|---------|------|
| **ActorRollout** | ç­–ç•¥ç”Ÿæˆ + Actor è®­ç»ƒ | âœ… å¿…éœ€ | æ··åˆå¼•æ“æ¨¡å¼ |
| **Critic** | ä»·å€¼ç½‘ç»œè®­ç»ƒ | âŒ å¯é€‰ | GAE éœ€è¦ï¼ŒGRPO ä¸éœ€è¦ |
| **RefPolicy** | å‚è€ƒç­–ç•¥æ¨ç† | âŒ å¯é€‰ | è®¡ç®— KL æ•£åº¦æ—¶éœ€è¦ |
| **RewardModel** | å¥–åŠ±æ¨¡å‹æ¨ç† | âŒ å¯é€‰ | åŸºäºæ¨¡å‹çš„å¥–åŠ± |

### 2.3 æ•°æ®æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataloader â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ batch
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              1. Generate Phase                    â”‚
â”‚  ActorRollout.generate_sequences()               â”‚
â”‚    Input: prompts                                â”‚
â”‚    Output: responses                             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. Reward Phase                      â”‚
â”‚  RewardModel.compute_rm_score() [optional]       â”‚
â”‚  compute_reward(reward_fn)                       â”‚
â”‚    Output: token_level_scores                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         3. Log Probability Phase                  â”‚
â”‚  ActorRollout.compute_log_prob()                 â”‚
â”‚    Output: old_log_probs                         â”‚
â”‚  RefPolicy.compute_ref_log_prob() [optional]     â”‚
â”‚    Output: ref_log_probs                         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4. Value Phase                       â”‚
â”‚  Critic.compute_values() [optional]              â”‚
â”‚    Output: values                                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           5. Advantage Phase                      â”‚
â”‚  compute_advantage() [on driver]                 â”‚
â”‚    Input: rewards, values, masks                 â”‚
â”‚    Output: advantages, returns                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                      â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Update    â”‚    â”‚ 7. Update    â”‚    â”‚ 8. Validate  â”‚
â”‚    Critic    â”‚    â”‚    Actor     â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. å¯åŠ¨æµç¨‹è¯¦è§£

### 3.1 å¯åŠ¨å…¥å£

**æ–‡ä»¶**: `verl/trainer/main_ppo.py`

```python
@hydra.main(config_path="config", config_name="ppo_trainer", version_base=None)
def main(config):
    """ä¸»å…¥å£å‡½æ•°"""
    run_ppo(config)
```

**è°ƒç”¨æ–¹å¼**:
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="['/path/to/data']" \
    actor_rollout_ref.model.path=/path/to/model
```

### 3.2 ç¬¬ä¸€é˜¶æ®µ: Ray é›†ç¾¤åˆå§‹åŒ–

**å‡½æ•°**: `run_ppo(config)`

```python
def run_ppo(config, task_runner_class=None) -> None:
    # æ­¥éª¤ 1: æ£€æŸ¥å¹¶åˆå§‹åŒ– Ray
    if not ray.is_initialized():
        default_runtime_env = get_ppo_ray_runtime_env()
        # è®¾ç½®ç¯å¢ƒå˜é‡
        ray_init_kwargs = {
            "runtime_env": {
                "env_vars": {
                    "TOKENIZERS_PARALLELISM": "true",
                    "NCCL_DEBUG": "WARN",
                    "VLLM_LOGGING_LEVEL": "INFO",
                    # ...
                }
            },
            "num_cpus": config.trainer.n_gpus_per_node * config.trainer.nnodes
        }
        ray.init(**ray_init_kwargs)
    
    # æ­¥éª¤ 2: åˆ›å»º TaskRunner (Ray Remote Actor)
    if task_runner_class is None:
        task_runner_class = ray.remote(num_cpus=1)(TaskRunner)
    
    runner = task_runner_class.remote()
    
    # æ­¥éª¤ 3: æ‰§è¡Œè®­ç»ƒä»»åŠ¡
    ray.get(runner.run.remote(config))
```

**å…³é”®ç‚¹**:
- âœ“ Ray è¿è¡Œæ—¶ç¯å¢ƒé…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰
- âœ“ TaskRunner ä½œä¸º Ray Remote Actor è¿è¡Œ
- âœ“ é€šè¿‡ `ray.get()` ç­‰å¾…è®­ç»ƒå®Œæˆ

### 3.3 ç¬¬äºŒé˜¶æ®µ: TaskRunner åˆå§‹åŒ–

**ç±»**: `TaskRunner`

```python
class TaskRunner:
    def __init__(self):
        self.role_worker_mapping = {}  # Role -> Worker Class
        self.mapping = {}              # Role -> Resource Pool ID
    
    def run(self, config):
        # æ­¥éª¤ 1: æ³¨å†Œ Worker Classes
        self.add_actor_rollout_worker(config)
        self.add_critic_worker(config)
        self.add_reward_model_worker(config)
        self.add_ref_policy_worker(config)
        
        # æ­¥éª¤ 2: åˆå§‹åŒ–èµ„æºæ± 
        resource_pool_manager = self.init_resource_pool_mgr(config)
        
        # æ­¥éª¤ 3: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        local_path = copy_to_local(config.actor_rollout_ref.model.path)
        tokenizer = hf_tokenizer(local_path)
        processor = hf_processor(local_path)
        
        # æ­¥éª¤ 4: åŠ è½½å¥–åŠ±å‡½æ•°
        reward_fn = load_reward_manager(config, tokenizer)
        val_reward_fn = load_reward_manager(config, tokenizer)
        
        # æ­¥éª¤ 5: åˆ›å»ºæ•°æ®é›†
        train_dataset = create_rl_dataset(...)
        val_dataset = create_rl_dataset(...)
        train_sampler = create_rl_sampler(...)
        
        # æ­¥éª¤ 6: åˆ›å»º Trainer
        trainer = RayPPOTrainer(
            config=config,
            tokenizer=tokenizer,
            role_worker_mapping=self.role_worker_mapping,
            resource_pool_manager=resource_pool_manager,
            reward_fn=reward_fn,
            train_dataset=train_dataset,
            ...
        )
        
        # æ­¥éª¤ 7: åˆå§‹åŒ–åˆ†å¸ƒå¼ Workers
        trainer.init_workers()
        
        # æ­¥éª¤ 8: å¼€å§‹è®­ç»ƒ
        trainer.fit()
```

#### 3.3.1 Worker æ³¨å†Œæœºåˆ¶

```python
def add_actor_rollout_worker(self, config):
    """æ³¨å†Œ ActorRollout Worker"""
    if config.actor_rollout_ref.actor.strategy in {"fsdp", "fsdp2"}:
        from verl.workers.fsdp_workers import ActorRolloutRefWorker
        actor_rollout_cls = ActorRolloutRefWorker
    elif config.actor_rollout_ref.actor.strategy == "megatron":
        from verl.workers.megatron_workers import ActorRolloutRefWorker
        actor_rollout_cls = ActorRolloutRefWorker
    
    self.role_worker_mapping[Role.ActorRollout] = ray.remote(actor_rollout_cls)
    return actor_rollout_cls
```

#### 3.3.2 èµ„æºæ± åˆå§‹åŒ–

```python
def init_resource_pool_mgr(self, config):
    """åˆå§‹åŒ– GPU èµ„æºæ± """
    resource_pool_spec = {
        "global_pool": [
            config.trainer.n_gpus_per_node  # æ¯ä¸ªèŠ‚ç‚¹ GPU æ•°
        ] * config.trainer.nnodes           # èŠ‚ç‚¹æ•°
    }
    
    # å¯é€‰ï¼šä¸º RewardModel åˆ›å»ºç‹¬ç«‹èµ„æºæ± 
    if config.reward_model.enable_resource_pool:
        reward_pool = [
            config.reward_model.n_gpus_per_node
        ] * config.reward_model.nnodes
        resource_pool_spec["reward_pool"] = reward_pool
    
    # Role -> Resource Pool æ˜ å°„
    self.mapping[Role.ActorRollout] = "global_pool"
    self.mapping[Role.Critic] = "global_pool"
    
    return ResourcePoolManager(
        resource_pool_spec=resource_pool_spec,
        mapping=self.mapping
    )
```

### 3.4 ç¬¬ä¸‰é˜¶æ®µ: RayPPOTrainer åˆå§‹åŒ–

```python
class RayPPOTrainer:
    def __init__(self, config, tokenizer, role_worker_mapping, 
                 resource_pool_manager, ...):
        self.config = config
        self.tokenizer = tokenizer
        self.role_worker_mapping = role_worker_mapping
        self.resource_pool_manager = resource_pool_manager
        
        # æ£€æŸ¥éœ€è¦çš„ç»„ä»¶
        self.use_critic = need_critic(config)
        self.use_reference_policy = need_reference_policy(role_worker_mapping)
        self.use_rm = need_reward_model(role_worker_mapping)
        
        # åˆå§‹åŒ– KL æ§åˆ¶å™¨
        if config.algorithm.use_kl_in_reward:
            self.kl_ctrl_in_reward = get_kl_controller(config.algorithm.kl_ctrl)
        
        # åˆ›å»º DataLoader
        self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)
```

#### 3.4.1 Worker åˆå§‹åŒ–

```python
def init_workers(self):
    """åˆå§‹åŒ–æ‰€æœ‰åˆ†å¸ƒå¼ Workers"""
    # æ­¥éª¤ 1: åˆ›å»ºèµ„æºæ± 
    self.resource_pool_manager.create_resource_pool()
    
    # æ­¥éª¤ 2: ä¸ºæ¯ä¸ª Role åˆ›å»º RayClassWithInitArgs
    resource_pool_to_cls = {}
    
    # ActorRollout
    resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
    actor_rollout_cls = RayClassWithInitArgs(
        cls=self.role_worker_mapping[Role.ActorRollout],
        config=self.config.actor_rollout_ref,
        role=str(Role.ActorRollout),
    )
    resource_pool_to_cls[resource_pool][str(Role.ActorRollout)] = actor_rollout_cls
    
    # Critic (å¦‚æœéœ€è¦)
    if self.use_critic:
        critic_cls = RayClassWithInitArgs(
            cls=self.role_worker_mapping[Role.Critic],
            config=self.config.critic
        )
        resource_pool_to_cls[resource_pool][str(Role.Critic)] = critic_cls
    
    # æ­¥éª¤ 3: åˆ›å»º WorkerGroups
    all_wg = {}
    for resource_pool, class_dict in resource_pool_to_cls.items():
        worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
        wg_dict = RayWorkerGroup(
            resource_pool=resource_pool,
            ray_cls_with_init=worker_dict_cls,
        )
        spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
        all_wg.update(spawn_wg)
    
    # æ­¥éª¤ 4: åˆå§‹åŒ–æ¨¡å‹
    self.actor_rollout_wg = all_wg[str(Role.ActorRollout)]
    self.actor_rollout_wg.init_model()
    
    if self.use_critic:
        self.critic_wg = all_wg[str(Role.Critic)]
        self.critic_wg.init_model()
    
    if self.use_reference_policy:
        self.ref_policy_wg = all_wg[str(Role.RefPolicy)]
        self.ref_policy_wg.init_model()
```

---

## 4. æ ¸å¿ƒç»„ä»¶è§£æ

### 4.1 ResourcePoolManager

**èŒè´£**: ç®¡ç† GPU èµ„æºåˆ†é…

```python
@dataclass
class ResourcePoolManager:
    resource_pool_spec: dict[str, list[int]]  # èµ„æºæ± è§„æ ¼
    mapping: dict[Role, str]                  # Role -> èµ„æºæ± æ˜ å°„
    resource_pool_dict: dict[str, RayResourcePool]
    
    def create_resource_pool(self):
        """åˆ›å»º Ray èµ„æºæ± """
        for pool_name, process_on_nodes in self.resource_pool_spec.items():
            resource_pool = RayResourcePool(
                process_on_nodes=process_on_nodes,
                use_gpu=True,
                max_colocate_count=1,  # FSDP å»ºè®®ä¸º 1
                name_prefix=pool_name
            )
            self.resource_pool_dict[pool_name] = resource_pool
```

**ç¤ºä¾‹é…ç½®**:
```yaml
# å•æœº 8 å¡
resource_pool_spec:
  global_pool: [8]

# åŒæœº 16 å¡
resource_pool_spec:
  global_pool: [8, 8]

# ç‹¬ç«‹å¥–åŠ±æ¨¡å‹æ± 
resource_pool_spec:
  global_pool: [8]
  reward_pool: [4]
```

### 4.2 RayWorkerGroup

**èŒè´£**: ç®¡ç†åŒä¸€ Role çš„å¤šä¸ª Workers

```python
class RayWorkerGroup(WorkerGroup):
    def __init__(self, resource_pool, ray_cls_with_init):
        self.resource_pool = resource_pool
        self.workers = []  # Ray Actor handles
        self.world_size = 0
    
    def spawn(self, prefix_set):
        """åˆ›å»ºåˆ†å¸ƒå¼ Workers"""
        pgs = self.resource_pool.get_placement_groups()
        
        for pg in pgs:
            for bundle_idx in range(len(pg.bundle_specs)):
                # åˆ›å»º Ray Actor
                worker = self.ray_cls_with_init.remote()
                self.workers.append(worker)
        
        self.world_size = len(self.workers)
        return self
    
    def init_model(self):
        """åœ¨æ‰€æœ‰ Workers ä¸Šåˆå§‹åŒ–æ¨¡å‹"""
        ray.get([worker.init_model.remote() for worker in self.workers])
    
    def generate_sequences(self, batch: DataProto):
        """åˆ†å¸ƒå¼åºåˆ—ç”Ÿæˆ"""
        # åˆ†å‘æ•°æ®åˆ°å„ä¸ª workers
        outputs = [
            worker.generate_sequences.remote(batch_shard)
            for worker, batch_shard in zip(self.workers, batch.split())
        ]
        # æ”¶é›†ç»“æœ
        results = ray.get(outputs)
        return DataProto.concat(results)
```

### 4.3 DataProto

**èŒè´£**: ç»Ÿä¸€çš„åˆ†å¸ƒå¼æ•°æ®ä¼ è¾“åè®®

```python
@dataclass
class DataProto:
    batch: dict[str, torch.Tensor]           # å¼ é‡æ•°æ®
    non_tensor_batch: dict[str, np.ndarray]  # éå¼ é‡æ•°æ®
    meta_info: dict[str, Any]                # å…ƒä¿¡æ¯
    
    def split(self, n_parts: int):
        """æŒ‰ DP ç»´åº¦åˆ‡åˆ†"""
        ...
    
    def concat(parts: list):
        """åˆå¹¶å¤šä¸ª DataProto"""
        ...
    
    def union(self, other):
        """åˆå¹¶ä¸¤ä¸ª DataProto"""
        ...
```

---

## 5. PPO è®­ç»ƒå¾ªç¯

### 5.1 å®Œæ•´è®­ç»ƒæ­¥éª¤

```python
def fit(self):
    """PPO è®­ç»ƒä¸»å¾ªç¯"""
    logger = Tracking(...)
    self.global_steps = 0
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    self._load_checkpoint()
    
    # è®­ç»ƒå‰éªŒè¯
    if self.val_reward_fn is not None:
        val_metrics = self._validate()
        logger.log(data=val_metrics, step=self.global_steps)
    
    # ä¸»å¾ªç¯
    for epoch in range(self.config.trainer.total_epochs):
        for batch_dict in self.train_dataloader:
            self.global_steps += 1
            batch = DataProto.from_single_dict(batch_dict)
            
            # ===== æ­¥éª¤ 1: ç”Ÿæˆ =====
            gen_batch_output = self.actor_rollout_wg.generate_sequences(batch)
            
            # ===== æ­¥éª¤ 2: è®¡ç®—å¥–åŠ± =====
            if self.use_rm:
                rm_scores = self.rm_wg.compute_rm_score(batch)
                batch = batch.union(rm_scores)
            reward_tensor, reward_extra_info = compute_reward(batch, self.reward_fn)
            batch.batch["token_level_scores"] = reward_tensor
            
            # ===== æ­¥éª¤ 3: è®¡ç®—å¯¹æ•°æ¦‚ç‡ =====
            old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
            batch = batch.union(old_log_prob)
            
            if self.use_reference_policy:
                ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)
                batch = batch.union(ref_log_prob)
            
            # ===== æ­¥éª¤ 4: è®¡ç®—ä»·å€¼ =====
            if self.use_critic:
                values = self.critic_wg.compute_values(batch)
                batch = batch.union(values)
            
            # ===== æ­¥éª¤ 5: è®¡ç®—ä¼˜åŠ¿ (Driver) =====
            if self.config.algorithm.use_kl_in_reward:
                batch, kl_metrics = apply_kl_penalty(batch, self.kl_ctrl_in_reward)
            else:
                batch.batch["token_level_rewards"] = batch.batch["token_level_scores"]
            
            batch = compute_advantage(
                batch,
                adv_estimator=self.config.algorithm.adv_estimator,
                gamma=self.config.algorithm.gamma,
                lam=self.config.algorithm.lam,
            )
            
            # ===== æ­¥éª¤ 6: æ›´æ–° Critic =====
            if self.use_critic:
                critic_output = self.critic_wg.update_critic(batch)
            
            # ===== æ­¥éª¤ 7: æ›´æ–° Actor =====
            actor_output = self.actor_rollout_wg.update_actor(batch)
            
            # ===== æ­¥éª¤ 8: éªŒè¯å’Œä¿å­˜ =====
            if self.global_steps % self.config.trainer.test_freq == 0:
                val_metrics = self._validate()
            
            if self.global_steps % self.config.trainer.save_freq == 0:
                self._save_checkpoint()
            
            # è®°å½•æŒ‡æ ‡
            logger.log(data=metrics, step=self.global_steps)
```

### 5.2 ä¼˜åŠ¿è®¡ç®—è¯¦è§£

```python
def compute_advantage(data, adv_estimator, gamma, lam):
    """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
    
    if adv_estimator == AdvantageEstimator.GAE:
        # Generalized Advantage Estimation
        advantages, returns = compute_gae_advantage_return(
            token_level_rewards=data.batch["token_level_rewards"],
            values=data.batch["values"],
            response_mask=data.batch["response_mask"],
            gamma=gamma,  # æŠ˜æ‰£å› å­
            lam=lam,      # GAE Î»
        )
    
    elif adv_estimator == AdvantageEstimator.GRPO:
        # Group Relative Policy Optimization
        advantages, returns = compute_grpo_outcome_advantage(
            token_level_rewards=data.batch["token_level_rewards"],
            response_mask=data.batch["response_mask"],
            index=data.non_tensor_batch["uid"],
        )
    
    elif adv_estimator == AdvantageEstimator.REINFORCE_PLUS_PLUS:
        # REINFORCE++
        advantages, returns = compute_reinforce_plus_plus_advantage(
            token_level_rewards=data.batch["token_level_rewards"],
            response_mask=data.batch["response_mask"],
        )
    
    data.batch["advantages"] = advantages
    data.batch["returns"] = returns
    return data
```

### 5.3 Actor æ›´æ–°æœºåˆ¶

```python
# åœ¨ ActorRolloutRefWorker ä¸­
def update_actor(self, data: DataProto):
    """æ›´æ–° Actor ç­–ç•¥"""
    
    # æ­¥éª¤ 1: å‰å‘ä¼ æ’­è®¡ç®—æ–°çš„ log_probs
    log_probs = self.model.compute_log_prob(
        input_ids=data.batch["input_ids"],
        attention_mask=data.batch["attention_mask"],
        responses=data.batch["responses"],
    )
    
    # æ­¥éª¤ 2: è®¡ç®— PPO æŸå¤±
    policy_loss = compute_policy_loss(
        old_log_prob=data.batch["old_log_probs"],
        log_prob=log_probs,
        advantages=data.batch["advantages"],
        response_mask=data.batch["response_mask"],
        clip_ratio=self.config.clip_ratio,
    )
    
    # æ­¥éª¤ 3: åå‘ä¼ æ’­å’Œä¼˜åŒ–
    self.optimizer.zero_grad()
    policy_loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(
        self.model.parameters(),
        max_norm=self.config.max_grad_norm
    )
    
    self.optimizer.step()
    
    return DataProto(meta_info={"metrics": {"loss": policy_loss.item()}})
```

---

## 6. é…ç½®ç³»ç»Ÿ

### 6.1 Hydra é…ç½®ç»“æ„

```
verl/trainer/config/
â”œâ”€â”€ ppo_trainer.yaml          # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ algorithm/
â”‚   â”œâ”€â”€ ppo.yaml              # PPO ç®—æ³•é…ç½®
â”‚   â””â”€â”€ grpo.yaml             # GRPO ç®—æ³•é…ç½®
â”œâ”€â”€ actor_rollout_ref/
â”‚   â”œâ”€â”€ fsdp.yaml             # FSDP ç­–ç•¥
â”‚   â””â”€â”€ megatron.yaml         # Megatron ç­–ç•¥
â”œâ”€â”€ critic/
â”‚   â””â”€â”€ fsdp.yaml
â””â”€â”€ data/
    â””â”€â”€ rlhf.yaml
```

### 6.2 æ ¸å¿ƒé…ç½®é¡¹

#### 6.2.1 ç®—æ³•é…ç½®

```yaml
algorithm:
  adv_estimator: grpo              # gae, grpo, reinforce_plus_plus
  gamma: 1.0                        # æŠ˜æ‰£å› å­
  lam: 0.95                         # GAE Î»
  
  # KL æ•£åº¦æ§åˆ¶
  use_kl_in_reward: false           # æ˜¯å¦åœ¨å¥–åŠ±ä¸­åŠ  KL æƒ©ç½š
  kl_penalty: kl                    # kl, abs, mse
  kl_ctrl:
    kl_coef: 0.01                   # KL ç³»æ•°
    target_kl: 0.1                  # ç›®æ ‡ KL
```

#### 6.2.2 Actor é…ç½®

```yaml
actor_rollout_ref:
  model:
    path: /path/to/model            # æ¨¡å‹è·¯å¾„
    lora_rank: 0                    # LoRA rank (0=å…¨å‚æ•°)
    use_remove_padding: true        # ç§»é™¤ padding ä¼˜åŒ–
    enable_gradient_checkpointing: true
  
  actor:
    strategy: fsdp                  # fsdp, fsdp2, megatron
    
    # PPO å‚æ•°
    ppo_mini_batch_size: 16         # Mini-batch å¤§å°
    ppo_epochs: 1                   # PPO epochs
    clip_ratio_low: 0.8             # PPO clip ä¸‹ç•Œ
    clip_ratio_high: 1.2            # PPO clip ä¸Šç•Œ
    
    # ä¼˜åŒ–å™¨
    optim:
      lr: 1e-6                      # å­¦ä¹ ç‡
      warmup_steps: 10              # é¢„çƒ­æ­¥æ•°
      total_training_steps: 1000    # æ€»è®­ç»ƒæ­¥æ•°
    
    # FSDP é…ç½®
    fsdp_config:
      param_offload: true           # å‚æ•°å¸è½½åˆ° CPU
      optimizer_offload: true       # ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½
      ulysses_sequence_parallel_size: 4  # åºåˆ—å¹¶è¡Œ
  
  rollout:
    name: vllm                      # vllm, sglang
    mode: async                     # sync, async
    tensor_model_parallel_size: 4   # TP å¤§å°
    n: 16                           # æ¯ä¸ª prompt ç”Ÿæˆæ•°
    temperature: 1.0
    top_p: 0.9
    max_new_tokens: 2048
    
    # å¤šè½®å¯¹è¯é…ç½®
    multi_turn:
      enable: true
      max_user_turns: 16
      max_assistant_turns: 16
      tool_config_path: path/to/tools.yaml
```

#### 6.2.3 Critic é…ç½®

```yaml
critic:
  strategy: fsdp
  ppo_mini_batch_size: 16
  ppo_epochs: 1
  
  optim:
    lr: 5e-6
    warmup_steps: 10
  
  fsdp_config:
    param_offload: true
    optimizer_offload: true
```

#### 6.2.4 æ•°æ®é…ç½®

```yaml
data:
  train_files: ['/path/to/train.parquet']
  val_files: ['/path/to/val.parquet']
  
  train_batch_size: 64
  val_batch_size: 32
  max_prompt_length: 2048
  max_response_length: 2048
  
  shuffle: true
  dataloader_num_workers: 8
  
  # è‡ªå®šä¹‰æ•°æ®é›†
  custom_cls:
    path: recipe/custom/dataset.py
    name: CustomRLHFDataset
```

#### 6.2.5 è®­ç»ƒé…ç½®

```yaml
trainer:
  nnodes: 1                         # èŠ‚ç‚¹æ•°
  n_gpus_per_node: 8                # æ¯èŠ‚ç‚¹ GPU æ•°
  
  total_epochs: 1
  total_training_steps: 1000
  
  # ä¿å­˜å’ŒéªŒè¯
  save_freq: 100                    # ä¿å­˜é¢‘ç‡
  test_freq: 50                     # éªŒè¯é¢‘ç‡
  val_before_train: true            # è®­ç»ƒå‰éªŒè¯
  
  # æ£€æŸ¥ç‚¹
  default_local_dir: ./checkpoints
  default_hdfs_dir: null
  resume_mode: auto                 # auto, disable, resume_path
  
  # æ—¥å¿—
  logger: [console, wandb]
  project_name: my_project
  experiment_name: my_exp
  log_val_generations: 20           # è®°å½•éªŒè¯æ ·æœ¬æ•°
```

---

## 7. å®é™…æ¡ˆä¾‹åˆ†æ

### 7.1 æ¡ˆä¾‹: Qwen2.5-7B DAPO è®­ç»ƒ

**é…ç½®æ–‡ä»¶**: `recipe/retool/run_qwen2_7b_dapo.sh`

#### 7.1.1 ç¡¬ä»¶é…ç½®

```bash
# å•æœº 8 å¡ A100/H100
nnodes=1
n_gpus_per_node=8
```

#### 7.1.2 æ¨¡å‹é…ç½®

```bash
model_path=checkpoint/multiturn-sft-qwen-2.5-7b-instruct/global_step_372

# æ¨ç†ï¼šTP=4 (vLLM)
infer_tp=4

# è®­ç»ƒï¼šSP=4 (Sequence Parallel)
train_sp=4

# å†…å­˜ä¼˜åŒ–ï¼šå¸è½½åˆ° CPU
offload=True
```

#### 7.1.3 ç®—æ³•é…ç½®

```bash
# ç®—æ³•ï¼šGroup Relative Policy Optimization
adv_estimator=grpo

# ä¸ä½¿ç”¨ KL æ•£åº¦
use_kl_in_reward=False
use_kl_loss=False

# PPO clip èŒƒå›´
clip_ratio_low=0.2
clip_ratio_high=0.28
```

#### 7.1.4 æ•°æ®é…ç½®

```bash
# æ•°æ®é›†
train_files="['$DATA_ROOT/dataset/BytedTsinghua-SIA/DAPO-Math-17k']"
test_files="['$DATA_ROOT/dataset/yentinglin/aime_2025']"

# Batch é…ç½®
train_batch_size=64
ppo_mini_batch_size=16
n_resp_per_prompt=16
n_resp_per_prompt_val=30

# åºåˆ—é•¿åº¦
max_prompt_length=2048
max_response_length=16384  # æ”¯æŒé•¿æ–‡æœ¬ç”Ÿæˆ
```

#### 7.1.5 å¤šè½®å¯¹è¯é…ç½®

```bash
# å¯ç”¨å¤šè½®å·¥å…·è°ƒç”¨
multi_turn.enable=True
multi_turn.max_user_turns=16
multi_turn.max_assistant_turns=16
multi_turn.tool_config_path=recipe/retool/sandbox_fusion_tool_config.yaml
multi_turn.format=hermes
```

#### 7.1.6 æ€§èƒ½ä¼˜åŒ–

```bash
# vLLM é…ç½®
rollout.name=vllm
rollout.mode=async                    # å¼‚æ­¥ç”Ÿæˆ
rollout.gpu_memory_utilization=0.9    # GPU æ˜¾å­˜åˆ©ç”¨ç‡

# åŠ¨æ€ Batch Size
actor.use_dynamic_bsz=True

# åºåˆ—å¹¶è¡Œ
actor.ulysses_sequence_parallel_size=4

# å†…å­˜å¸è½½
actor.fsdp_config.param_offload=True
actor.fsdp_config.optimizer_offload=True

# Remove Padding ä¼˜åŒ–
model.use_remove_padding=True

# Gradient Checkpointing
model.enable_gradient_checkpointing=True
```

### 7.2 èµ„æºåˆ†é…ç¤ºæ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           8x A100 (80GB each)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ vLLM Engine   â”‚     â”‚ FSDP Engine  â”‚    â”‚
â”‚  â”‚  (TP=4)       â”‚     â”‚  (SP=4)      â”‚    â”‚
â”‚  â”‚               â”‚     â”‚              â”‚    â”‚
â”‚  â”‚  GPU 0-3      â”‚     â”‚  GPU 0-7     â”‚    â”‚
â”‚  â”‚  æ¨ç†ç”Ÿæˆ      â”‚     â”‚  è®­ç»ƒä¼˜åŒ–     â”‚    â”‚
â”‚  â”‚               â”‚     â”‚              â”‚    â”‚
â”‚  â”‚ ~40GB/GPU     â”‚     â”‚ ~70GB/GPU    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                             â”‚
â”‚  Hybrid Engine: ç”Ÿæˆå’Œè®­ç»ƒå…±äº« GPU           â”‚
â”‚  - GPU 0-3: ç”Ÿæˆæ—¶ç”¨ vLLM, è®­ç»ƒæ—¶ç”¨ FSDP    â”‚
â”‚  - GPU 4-7: ä»…ç”¨äº FSDP è®­ç»ƒ               â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 è®­ç»ƒæµç¨‹æ—¶åºå›¾

```
æ—¶é—´è½´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

Step 1: Generate (vLLM on GPU 0-3)
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ ~5s
         â”‚
Step 2: Reward Computation (CPU)
         â”‚â–ˆâ–ˆâ–ˆâ”‚ ~0.5s
            â”‚
Step 3: Compute Log Probs (FSDP on GPU 0-7)
            â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚ ~2s
                â”‚
Step 4: Compute Advantage (CPU)
                â”‚â–ˆâ–ˆâ”‚ ~0.5s
                  â”‚
Step 5: Update Actor (FSDP on GPU 0-7)
                  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ ~3s
                        â”‚
Step 6: Validation (æ¯ 10 æ­¥)
                        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ ~8s
                                    â”‚
æ€»è€—æ—¶: ~11s/step (è®­ç»ƒæ­¥)
       ~19s/step (å«éªŒè¯æ­¥)
```

---

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 å†…å­˜ä¼˜åŒ–

#### 8.1.1 å¸è½½ç­–ç•¥

```yaml
actor:
  fsdp_config:
    # å‚æ•°å¸è½½ï¼šè®­ç»ƒæ—¶å°†å‚æ•°å¸è½½åˆ° CPU
    param_offload: true
    
    # ä¼˜åŒ–å™¨å¸è½½ï¼šå°† optimizer states å¸è½½åˆ° CPU
    optimizer_offload: true
```

**æ•ˆæœ**: å¯èŠ‚çœ 40-60% GPU æ˜¾å­˜

#### 8.1.2 Remove Padding

```yaml
model:
  use_remove_padding: true
```

**åŸç†**: ç§»é™¤ padding tokensï¼Œåªè®¡ç®—æœ‰æ•ˆ tokens  
**æ•ˆæœ**: èŠ‚çœ 20-30% æ˜¾å­˜å’Œè®¡ç®—

#### 8.1.3 Gradient Checkpointing

```yaml
model:
  enable_gradient_checkpointing: true
```

**æ•ˆæœ**: ç”¨è®¡ç®—æ¢æ˜¾å­˜ï¼Œå¯èŠ‚çœ 50% æ¿€æ´»å€¼æ˜¾å­˜

### 8.2 è®¡ç®—ä¼˜åŒ–

#### 8.2.1 Sequence Parallel

```yaml
actor:
  ulysses_sequence_parallel_size: 4
```

**é€‚ç”¨åœºæ™¯**: é•¿åºåˆ—è®­ç»ƒ (>8K tokens)  
**æ•ˆæœ**: çº¿æ€§æ‰©å±•åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›

#### 8.2.2 Tensor Parallel (TP)

```yaml
rollout:
  tensor_model_parallel_size: 4
```

**é€‚ç”¨åœºæ™¯**: å¤§æ¨¡å‹æ¨ç† (>13B)  
**æ•ˆæœ**: é™ä½å•å¡æ˜¾å­˜éœ€æ±‚ï¼Œæé«˜æ¨ç†åå

#### 8.2.3 Dynamic Batch Size

```yaml
actor:
  use_dynamic_bsz: true
```

**åŸç†**: æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´ batch size  
**æ•ˆæœ**: å……åˆ†åˆ©ç”¨ GPU ç®—åŠ›

### 8.3 å¼‚æ­¥ç”Ÿæˆä¼˜åŒ–

```yaml
rollout:
  mode: async  # å¼‚æ­¥ç”Ÿæˆæ¨¡å¼
  
  agent:
    num_workers: 4              # å¹¶è¡Œ worker æ•°
    max_concurrent_requests: 128 # æœ€å¤§å¹¶å‘è¯·æ±‚
```

**æ•ˆæœ**: æé«˜ç”Ÿæˆååï¼Œå‡å°‘ç­‰å¾…æ—¶é—´

### 8.4 Batch Balancing

```yaml
trainer:
  balance_batch: true
```

**åŸç†**: æ ¹æ®åºåˆ—é•¿åº¦å¹³è¡¡å„ DP rank çš„è®¡ç®—è´Ÿè½½  
**æ•ˆæœ**: å‡å°‘ stragglersï¼Œæé«˜è®­ç»ƒæ•ˆç‡

---

## 9. å¸¸è§é—®é¢˜

### 9.1 OOM (Out of Memory)

**é—®é¢˜**: GPU æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. å¯ç”¨å¸è½½
actor.fsdp_config.param_offload: true
actor.fsdp_config.optimizer_offload: true

# 2. å‡å° batch size
data.train_batch_size: 32  # ä» 64 é™åˆ° 32
actor.ppo_mini_batch_size: 8  # ä» 16 é™åˆ° 8

# 3. å¯ç”¨ gradient checkpointing
model.enable_gradient_checkpointing: true

# 4. å¢åŠ  TP/SP
rollout.tensor_model_parallel_size: 8
actor.ulysses_sequence_parallel_size: 8

# 5. é™ä½ vLLM æ˜¾å­˜åˆ©ç”¨ç‡
rollout.gpu_memory_utilization: 0.7  # ä» 0.9 é™åˆ° 0.7
```

### 9.2 è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**: æ¯æ­¥è€—æ—¶è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆ
rollout.mode: async

# 2. å‡å°‘éªŒè¯é¢‘ç‡
trainer.test_freq: 100  # ä» 10 å¢åŠ åˆ° 100

# 3. å‡å°‘ dataloader workers
data.dataloader_num_workers: 4  # ä» 8 é™åˆ° 4

# 4. ç¦ç”¨ä¸å¿…è¦çš„ç»„ä»¶
algorithm.use_kl_in_reward: false  # ä¸è®¡ç®— KL
critic: null  # GRPO ä¸éœ€è¦ critic
```

### 9.3 Ray åˆå§‹åŒ–å¤±è´¥

**é—®é¢˜**: Ray cluster å¯åŠ¨å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ Ray ç«¯å£
ray stop  # åœæ­¢å·²æœ‰ Ray è¿›ç¨‹
ray start --head  # é‡æ–°å¯åŠ¨

# 2. è®¾ç½®æ­£ç¡®çš„ num_cpus
ray_init:
  num_cpus: 96  # åº”ç­‰äºå®é™… CPU æ ¸æ•°

# 3. æ£€æŸ¥é˜²ç«å¢™
# ç¡®ä¿ Ray ç«¯å£ (6379, 8265) æœªè¢«å ç”¨
```

### 9.4 æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥

**é—®é¢˜**: æ— æ³•åŠ è½½ä¹‹å‰çš„æ£€æŸ¥ç‚¹

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. æ£€æŸ¥è·¯å¾„
trainer.default_local_dir: /absolute/path/to/checkpoints

# 2. æŒ‡å®šæ¢å¤è·¯å¾„
trainer.resume_mode: resume_path
trainer.resume_from_path: /path/to/checkpoint/global_step_100

# 3. ä»å¤´è®­ç»ƒ
trainer.resume_mode: disable
```

### 9.5 å¤šè½®å¯¹è¯å·¥å…·è°ƒç”¨å¤±è´¥

**é—®é¢˜**: Tool calling æ‰§è¡Œå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. æ£€æŸ¥å·¥å…·é…ç½®
rollout.multi_turn.tool_config_path: recipe/retool/sandbox_fusion_tool_config.yaml

# 2. æ£€æŸ¥æ ¼å¼
rollout.multi_turn.format: hermes  # æˆ– openai

# 3. æ£€æŸ¥ç¯å¢ƒ
# ç¡®ä¿æ²™ç®±ç¯å¢ƒæ­£å¸¸è¿è¡Œ
```

---

## é™„å½• A: å®Œæ•´é…ç½®ç¤ºä¾‹

### A.1 GRPO + vLLM + FSDP (æ¨è)

```yaml
# algorithm
algorithm:
  adv_estimator: grpo
  gamma: 1.0
  use_kl_in_reward: false

# data
data:
  train_files: ['/data/train.parquet']
  val_files: ['/data/val.parquet']
  train_batch_size: 64
  max_prompt_length: 2048
  max_response_length: 2048

# actor_rollout_ref
actor_rollout_ref:
  model:
    path: /models/qwen2.5-7b
    use_remove_padding: true
    enable_gradient_checkpointing: true
  
  actor:
    strategy: fsdp
    ppo_mini_batch_size: 16
    clip_ratio_low: 0.8
    clip_ratio_high: 1.2
    optim:
      lr: 1e-6
    fsdp_config:
      param_offload: true
      optimizer_offload: true
  
  rollout:
    name: vllm
    mode: async
    tensor_model_parallel_size: 4
    n: 16
    temperature: 1.0
    top_p: 0.9

# trainer
trainer:
  nnodes: 1
  n_gpus_per_node: 8
  total_epochs: 1
  save_freq: 100
  test_freq: 50
  logger: [console, wandb]
```

### A.2 PPO + GAE + Critic

```yaml
algorithm:
  adv_estimator: gae
  gamma: 0.99
  lam: 0.95
  use_kl_in_reward: true
  kl_ctrl:
    kl_coef: 0.01

critic:
  strategy: fsdp
  ppo_mini_batch_size: 16
  optim:
    lr: 5e-6
  fsdp_config:
    param_offload: true

# å…¶ä»–é…ç½®åŒä¸Š
```

---

## é™„å½• B: æ€§èƒ½åŸºå‡†

### B.1 å•æœº 8 å¡ A100 (80GB)

| æ¨¡å‹ | Batch Size | Seq Len | ååé‡ | æ˜¾å­˜å ç”¨ |
|------|-----------|---------|--------|---------|
| Qwen2.5-7B | 64 | 4096 | 150 samples/min | 75GB/GPU |
| Qwen2.5-14B | 32 | 4096 | 80 samples/min | 78GB/GPU |
| Qwen2.5-32B | 16 | 4096 | 35 samples/min | 79GB/GPU |

### B.2 ä¼˜åŒ–æ•ˆæœå¯¹æ¯”

| ä¼˜åŒ–é¡¹ | åŸºçº¿ | ä¼˜åŒ–å | æå‡ |
|-------|------|--------|------|
| Remove Padding | 100 samples/min | 130 samples/min | +30% |
| Async Rollout | 100 samples/min | 145 samples/min | +45% |
| Dynamic BSZ | 100 samples/min | 120 samples/min | +20% |
| ç»„åˆä¼˜åŒ– | 100 samples/min | 180 samples/min | +80% |

---

## æ€»ç»“

VERL æ˜¯ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„ RLHF è®­ç»ƒæ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹:

âœ… **çµæ´»çš„æ¶æ„**: æ”¯æŒå¤šç§åˆ†å¸ƒå¼ç­–ç•¥å’Œç®—æ³•  
âœ… **é«˜æ•ˆçš„æ€§èƒ½**: é€šè¿‡æ··åˆå¼•æ“å’Œå†…å­˜ä¼˜åŒ–å®ç°é«˜åå  
âœ… **æ˜“ç”¨çš„é…ç½®**: Hydra é…ç½®ç³»ç»Ÿç®€åŒ–å¤æ‚å‚æ•°ç®¡ç†  
âœ… **å®Œå–„çš„å·¥å…·**: å†…ç½®å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ç­‰é«˜çº§åŠŸèƒ½  

é€‚ç”¨äºä» 7B åˆ° 70B+ çš„å„ç§è§„æ¨¡æ¨¡å‹çš„ RLHF è®­ç»ƒã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-01-06  
**ä½œè€…**: VERL Team


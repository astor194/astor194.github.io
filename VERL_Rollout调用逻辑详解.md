# VERL Rollout è°ƒç”¨é€»è¾‘è¯¦è§£

## ğŸ“‹ ç›®å½•
- [1. Rollout æ¦‚è¿°](#1-rollout-æ¦‚è¿°)
- [2. è°ƒç”¨é“¾è·¯](#2-è°ƒç”¨é“¾è·¯)
- [3. åŒæ­¥æ¨¡å¼ (Sync Mode)](#3-åŒæ­¥æ¨¡å¼-sync-mode)
- [4. å¼‚æ­¥æ¨¡å¼ (Async Mode)](#4-å¼‚æ­¥æ¨¡å¼-async-mode)
- [5. æ··åˆå¼•æ“æœºåˆ¶](#5-æ··åˆå¼•æ“æœºåˆ¶)
- [6. å¤šè½®å¯¹è¯ä¸å·¥å…·è°ƒç”¨](#6-å¤šè½®å¯¹è¯ä¸å·¥å…·è°ƒç”¨)
- [7. å…·ä½“ä»£ç ç¤ºä¾‹](#7-å…·ä½“ä»£ç ç¤ºä¾‹)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)

---

## 1. Rollout æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ Rolloutï¼Ÿ

**Rollout** æ˜¯ PPO è®­ç»ƒä¸­è´Ÿè´£**åºåˆ—ç”Ÿæˆ**çš„ç»„ä»¶ã€‚å®ƒä½¿ç”¨å½“å‰ç­–ç•¥æ¨¡å‹ï¼ˆActorï¼‰ä»ç»™å®šçš„ prompt ç”Ÿæˆ responseã€‚

### 1.2 Rollout çš„ä½œç”¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompts   â”‚  "è§£å†³è¿™é“æ•°å­¦é¢˜ï¼š2+3=?"
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Rollout Engine            â”‚
â”‚  (vLLM/SGLang/HuggingFace)     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Responses  â”‚  "è§£ç­”ï¼š2+3=5"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ”¯æŒçš„ Rollout å¼•æ“

| å¼•æ“ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | æ¨¡å¼æ”¯æŒ |
|------|------|---------|---------|
| **vLLM** | é«˜ååã€PagedAttention | æ¨ç†ä¼˜åŒ– | sync, async |
| **SGLang** | RadixAttentionã€å¤šè½®ä¼˜åŒ– | å·¥å…·è°ƒç”¨ã€å¤šè½®å¯¹è¯ | sync, async |
| **HuggingFace** | ç®€å•æ˜“ç”¨ã€å…¼å®¹æ€§å¥½ | è°ƒè¯•ã€å°è§„æ¨¡ | sync |
| **Naive** | æœ€åŸºç¡€å®ç° | æµ‹è¯• | sync |

---

## 2. è°ƒç”¨é“¾è·¯

### 2.1 å®Œæ•´è°ƒç”¨æ ˆ

```
RayPPOTrainer.fit()
    â”‚
    â”œâ”€> actor_rollout_wg.generate_sequences(batch)
    â”‚       â”‚
    â”‚       â”œâ”€> [Ray RPC è°ƒç”¨æ‰€æœ‰ workers]
    â”‚       â”‚
    â”‚       â””â”€> ActorRolloutRefWorker.generate_sequences(prompts)
    â”‚               â”‚
    â”‚               â”œâ”€> [æ··åˆå¼•æ“] åˆ‡æ¢åˆ° rollout æ¨¡å¼
    â”‚               â”‚   await self.rollout_mode()
    â”‚               â”‚
    â”‚               â”œâ”€> self.rollout.generate_sequences(prompts)
    â”‚               â”‚       â”‚
    â”‚               â”‚       â”œâ”€> [vLLM] vLLMRollout.generate_sequences()
    â”‚               â”‚       â”œâ”€> [SGLang] SGLangRollout.generate_sequences()
    â”‚               â”‚       â””â”€> [HF] HFRollout.generate_sequences()
    â”‚               â”‚
    â”‚               â””â”€> [æ··åˆå¼•æ“] åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    â”‚                   await self.trainer_mode()
    â”‚
    â””â”€> è¿”å› DataProto(responses, log_probs, ...)
```

### 2.2 æ•°æ®æµè½¬

```python
# è¾“å…¥: DataProto
{
    "batch": {
        "input_ids": torch.Tensor,      # (batch_size, prompt_length)
        "attention_mask": torch.Tensor, # (batch_size, prompt_length)
        "position_ids": torch.Tensor,   # (batch_size, prompt_length)
    },
    "non_tensor_batch": {
        "uid": np.array,                # å”¯ä¸€æ ‡è¯†ç¬¦
        "data_source": np.array,        # æ•°æ®æ¥æº
    },
    "meta_info": {
        "eos_token_id": int,
        "pad_token_id": int,
        "do_sample": bool,
    }
}

# â†“ Rollout ç”Ÿæˆ â†“

# è¾“å‡º: DataProto
{
    "batch": {
        "prompts": torch.Tensor,            # (batch_size, prompt_length)
        "responses": torch.Tensor,          # (batch_size, response_length)
        "input_ids": torch.Tensor,          # (batch_size, total_length)
        "attention_mask": torch.Tensor,     # (batch_size, total_length)
        "position_ids": torch.Tensor,       # (batch_size, total_length)
        "response_mask": torch.Tensor,      # (batch_size, response_length)
        "rollout_log_probs": torch.Tensor,  # (batch_size, response_length) [å¯é€‰]
    },
    "non_tensor_batch": {...},
    "meta_info": {
        "timing": {
            "generate_sequences": float,    # ç”Ÿæˆè€—æ—¶
            "generation_timing/max": float,
            "generation_timing/min": float,
        }
    }
}
```

---

## 3. åŒæ­¥æ¨¡å¼ (Sync Mode)

### 3.1 é…ç½®

```yaml
actor_rollout_ref:
  rollout:
    name: vllm              # å¼•æ“ç±»å‹
    mode: sync              # åŒæ­¥æ¨¡å¼
    tensor_model_parallel_size: 4
    temperature: 1.0
    top_p: 0.9
    n: 16                   # æ¯ä¸ª prompt ç”Ÿæˆæ•°
```

### 3.2 å·¥ä½œæµç¨‹

```python
# åœ¨ ActorRolloutRefWorker ä¸­
def generate_sequences(self, prompts: DataProto):
    """åŒæ­¥ç”Ÿæˆåºåˆ—"""
    
    # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡
    prompts = prompts.to(device)
    
    # æ­¥éª¤ 2: [æ··åˆå¼•æ“] åˆ‡æ¢åˆ° Rollout æ¨¡å¼
    if self._is_actor:
        loop = get_event_loop()
        loop.run_until_complete(self.rollout_mode())
        # é‡Šæ”¾ Actor å‚æ•°ï¼ŒåŠ è½½ Rollout æƒé‡
    
    # æ­¥éª¤ 3: è°ƒç”¨ Rollout å¼•æ“ç”Ÿæˆ
    output = self.rollout.generate_sequences(prompts=prompts)
    
    # æ­¥éª¤ 4: [æ··åˆå¼•æ“] åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    if self._is_actor:
        loop.run_until_complete(self.trainer_mode())
        # é‡Šæ”¾ Rollout æƒé‡ï¼ŒåŠ è½½ Actor å‚æ•°
    
    # æ­¥éª¤ 5: è¿”å›ç»“æœ
    output = output.to("cpu")
    return output
```

### 3.3 vLLM åŒæ­¥ç”Ÿæˆç¤ºä¾‹

```python
class vLLMRollout(BaseRollout):
    def generate_sequences(self, prompts: DataProto, **kwargs) -> DataProto:
        """vLLM åŒæ­¥æ‰¹é‡ç”Ÿæˆ"""
        
        # 1. æå–è¾“å…¥
        idx = prompts.batch["input_ids"]           # (batch_size, prompt_length)
        attention_mask = prompts.batch["attention_mask"]
        position_ids = prompts.batch["position_ids"]
        batch_size = idx.shape[0]
        
        # 2. å‡†å¤‡ vLLM è¾“å…¥
        vllm_inputs = []
        for i in range(batch_size):
            # è¿‡æ»¤ padding tokens
            valid_mask = attention_mask[i] == 1
            valid_ids = idx[i][valid_mask].tolist()
            
            vllm_inputs.append(
                TokensPrompt(prompt_token_ids=valid_ids)
            )
        
        # 3. è°ƒç”¨ vLLM å¼•æ“
        with self.update_sampling_params(**kwargs):
            outputs = self.inference_engine.generate(
                prompts=vllm_inputs,
                sampling_params=self.sampling_params,
                use_tqdm=False,
            )
        
        # 4. è§£æè¾“å‡º
        response = []
        rollout_log_probs = []
        
        for output in outputs:
            for sample_id in range(len(output.outputs)):
                # æå–ç”Ÿæˆçš„ token IDs
                response_ids = output.outputs[sample_id].token_ids
                response.append(response_ids)
                
                # æå– log probabilities (å¦‚æœéœ€è¦)
                if self.config.calculate_log_probs:
                    curr_log_prob = []
                    for i, logprob in enumerate(output.outputs[sample_id].logprobs):
                        curr_log_prob.append(logprob[response_ids[i]].logprob)
                    rollout_log_probs.append(curr_log_prob)
        
        # 5. Padding åˆ°å›ºå®šé•¿åº¦
        response = pad_2d_list_to_length(
            response, 
            self.pad_token_id, 
            max_length=self.config.response_length
        ).to(idx.device)
        
        if self.config.calculate_log_probs:
            rollout_log_probs = pad_2d_list_to_length(
                rollout_log_probs, 
                -1, 
                max_length=self.config.response_length
            ).to(idx.device).float()
        
        # 6. æ„å»ºå®Œæ•´åºåˆ—
        seq = torch.cat([idx, response], dim=-1)
        
        # 7. æ„å»º position_ids
        response_length = response.size(1)
        delta_position_id = torch.arange(
            1, response_length + 1, device=position_ids.device
        ).unsqueeze(0).expand(batch_size, -1)
        
        new_position_ids = torch.cat([
            position_ids, 
            position_ids[:, -1:] + delta_position_id
        ], dim=-1)
        
        # 8. æ„å»º attention_mask
        response_attention_mask = (response != self.pad_token_id).long()
        new_attention_mask = torch.cat([
            attention_mask, 
            response_attention_mask
        ], dim=-1)
        
        # 9. è¿”å› DataProto
        batch = {
            "prompts": idx,
            "responses": response,
            "input_ids": seq,
            "attention_mask": new_attention_mask,
            "position_ids": new_position_ids,
            "response_mask": response_attention_mask,
        }
        
        if self.config.calculate_log_probs:
            batch["rollout_log_probs"] = rollout_log_probs
        
        return DataProto(batch=batch)
```

---

## 4. å¼‚æ­¥æ¨¡å¼ (Async Mode)

### 4.1 é…ç½®

```yaml
actor_rollout_ref:
  rollout:
    name: vllm
    mode: async             # å¼‚æ­¥æ¨¡å¼
    tensor_model_parallel_size: 4
    
    # å¼‚æ­¥å‚æ•°
    agent:
      num_workers: 4        # å¹¶å‘ worker æ•°
      max_concurrent_requests: 128  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    
    # å¤šè½®å¯¹è¯
    multi_turn:
      enable: true
      max_user_turns: 16
      max_assistant_turns: 16
```

### 4.2 AgentLoopManager æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AgentLoopManager                        â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      AsyncLLMServerManager                 â”‚    â”‚
â”‚  â”‚  (è´Ÿè½½å‡è¡¡ + Sticky Session)                â”‚    â”‚
â”‚  â”‚                                            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Server1 â”‚  â”‚ Server2 â”‚  â”‚ Server3 â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ (vLLM)  â”‚  â”‚ (vLLM)  â”‚  â”‚ (vLLM)  â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      AgentLoop (å¹¶å‘æ‰§è¡Œ)                   â”‚    â”‚
â”‚  â”‚                                            â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚ Loop 1   â”‚  â”‚ Loop 2   â”‚  â”‚ Loop N   â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ (Sample) â”‚  â”‚ (Sample) â”‚  â”‚ (Sample) â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Tool/Environment                      â”‚    â”‚
â”‚  â”‚  (Code Executor, Search, Calculator)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 å¼‚æ­¥ç”Ÿæˆæµç¨‹

```python
class AgentLoopManager:
    """ç®¡ç†å¼‚æ­¥ Rollout ç”Ÿæˆ"""
    
    def __init__(self, config, worker_group, rm_wg):
        self.config = config
        self.worker_group = worker_group
        
        # 1. åˆ›å»º LLM Servers (vLLM/SGLang)
        self.server_handles = self._create_servers()
        
        # 2. åˆ›å»º Server Manager (è´Ÿè½½å‡è¡¡)
        self.server_manager = AsyncLLMServerManager(
            config=config,
            server_handles=self.server_handles
        )
        
        # 3. åˆå§‹åŒ– Tokenizer/Processor
        self.tokenizer = hf_tokenizer(config.actor_rollout_ref.model.path)
        self.processor = hf_processor(config.actor_rollout_ref.model.path)
        
        # 4. åˆ›å»º AgentLoop ç±»
        self.agent_loop_cls = self._get_agent_loop_class()
    
    def generate_sequences(self, prompts: DataProto) -> DataProto:
        """å¼‚æ­¥å¹¶å‘ç”Ÿæˆ"""
        
        # 1. å°† batch è½¬æ¢ä¸ºå•ä¸ªæ ·æœ¬åˆ—è¡¨
        batch_size = len(prompts.batch["input_ids"])
        samples = []
        for i in range(batch_size):
            sample = {
                "input_ids": prompts.batch["input_ids"][i],
                "attention_mask": prompts.batch["attention_mask"][i],
                # ... å…¶ä»–å­—æ®µ
            }
            samples.append(sample)
        
        # 2. å¹¶å‘æ‰§è¡Œ AgentLoop
        loop = asyncio.get_event_loop()
        outputs = loop.run_until_complete(
            self._concurrent_generate(samples)
        )
        
        # 3. åˆå¹¶ç»“æœ
        return self._merge_outputs(outputs)
    
    async def _concurrent_generate(self, samples):
        """å¹¶å‘ç”Ÿæˆå¤šä¸ªæ ·æœ¬"""
        tasks = []
        for sample in samples:
            # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºä¸€ä¸ª AgentLoop å®ä¾‹
            agent_loop = self.agent_loop_cls(
                trainer_config=self.config,
                server_manager=self.server_manager,
                tokenizer=self.tokenizer,
                processor=self.processor,
            )
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            task = agent_loop.run(sample)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        outputs = await asyncio.gather(*tasks)
        return outputs
```

### 4.4 å•ä¸ª AgentLoop æ‰§è¡Œæµç¨‹

```python
class AgentLoopBase:
    """å•ä¸ªæ ·æœ¬çš„ Agent å¾ªç¯"""
    
    async def run(self, sample: dict) -> AgentLoopOutput:
        """æ‰§è¡Œå¤šè½®å¯¹è¯ + å·¥å…·è°ƒç”¨"""
        
        # åˆå§‹åŒ–
        request_id = str(uuid.uuid4())
        messages = sample["messages"]  # åˆå§‹å¯¹è¯
        
        prompt_ids = []
        response_ids = []
        response_mask = []
        response_logprobs = []
        num_turns = 0
        
        # å¤šè½®å¾ªç¯
        for turn in range(self.config.max_turns):
            num_turns += 1
            
            # æ­¥éª¤ 1: æ„å»º prompt
            prompt_text = self.tokenizer.apply_chat_template(
                messages, 
                add_generation_prompt=True
            )
            current_prompt_ids = self.tokenizer.encode(prompt_text)
            
            # æ­¥éª¤ 2: LLM ç”Ÿæˆ
            output = await self.server_manager.generate(
                request_id=request_id,
                prompt_ids=current_prompt_ids,
                sampling_params={
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                    "max_tokens": self.config.max_new_tokens,
                }
            )
            
            # æ­¥éª¤ 3: è§£æç”Ÿæˆç»“æœ
            generated_ids = output.token_ids
            generated_text = self.tokenizer.decode(generated_ids)
            
            # è®°å½• LLM ç”Ÿæˆçš„ tokens
            response_ids.extend(generated_ids)
            response_mask.extend([1] * len(generated_ids))
            if output.log_probs:
                response_logprobs.extend(output.log_probs)
            
            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "assistant",
                "content": generated_text
            })
            
            # æ­¥éª¤ 4: æ£€æµ‹å·¥å…·è°ƒç”¨
            tool_calls = self._parse_tool_calls(generated_text)
            
            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                break
            
            # æ­¥éª¤ 5: æ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["arguments"]
                
                # è°ƒç”¨å·¥å…·
                tool_result = await self._execute_tool(
                    tool_name, 
                    tool_args
                )
                
                # å°†å·¥å…·ç»“æœ tokenize
                tool_result_text = json.dumps(tool_result)
                tool_result_ids = self.tokenizer.encode(tool_result_text)
                
                # è®°å½•å·¥å…·è¿”å›çš„ tokens (mask=0)
                response_ids.extend(tool_result_ids)
                response_mask.extend([0] * len(tool_result_ids))
                if output.log_probs:
                    response_logprobs.extend([0.0] * len(tool_result_ids))
                
                # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
                messages.append({
                    "role": "tool",
                    "name": tool_name,
                    "content": tool_result_text
                })
        
        # æ­¥éª¤ 6: è®¡ç®—å¥–åŠ± (å¯é€‰)
        reward_score = None
        if self.config.compute_reward_in_rollout:
            reward_score = await self._compute_reward(
                prompt_ids, 
                response_ids
            )
        
        # è¿”å›ç»“æœ
        return AgentLoopOutput(
            prompt_ids=current_prompt_ids,
            response_ids=response_ids,
            response_mask=response_mask,
            response_logprobs=response_logprobs,
            reward_score=reward_score,
            num_turns=num_turns,
        )
```

### 4.5 è´Ÿè½½å‡è¡¡æœºåˆ¶

```python
class AsyncLLMServerManager:
    """LLM Server è´Ÿè½½å‡è¡¡å™¨"""
    
    def __init__(self, server_handles, max_cache_size=10000):
        self.server_handles = server_handles
        
        # æœ€å°‘è¯·æ±‚è´Ÿè½½å‡è¡¡ (Min-Heap)
        self.weighted_servers = [
            [0, (hash(server), server)] 
            for server in server_handles
        ]
        heapq.heapify(self.weighted_servers)
        
        # LRU ç¼“å­˜ï¼šrequest_id -> server
        # ç”¨äº Sticky Session (åŒä¸€ request_id æ€»æ˜¯å‘åˆ°åŒä¸€ server)
        self.request_id_to_server = LRUCache(maxsize=max_cache_size)
    
    def _choose_server(self, request_id: str):
        """é€‰æ‹© Server"""
        
        # å¦‚æœä¹‹å‰è®¿é—®è¿‡ï¼Œè¿”å›åŒä¸€ server (Sticky Session)
        if request_id in self.request_id_to_server:
            return self.request_id_to_server[request_id]
        
        # é€‰æ‹©è¯·æ±‚æ•°æœ€å°‘çš„ server
        server = self.weighted_servers[0][1][1]
        
        # æ›´æ–°è¯·æ±‚è®¡æ•°
        self.weighted_servers[0][0] += 1
        heapq.heapreplace(self.weighted_servers, self.weighted_servers[0])
        
        # ç¼“å­˜æ˜ å°„
        self.request_id_to_server[request_id] = server
        
        return server
    
    async def generate(self, request_id, prompt_ids, sampling_params):
        """ç”Ÿæˆåºåˆ—"""
        server = self._choose_server(request_id)
        
        output = await server.generate.remote(
            request_id=request_id,
            prompt_ids=prompt_ids,
            sampling_params=sampling_params,
        )
        
        return output
```

---

## 5. æ··åˆå¼•æ“æœºåˆ¶

### 5.1 ä»€ä¹ˆæ˜¯æ··åˆå¼•æ“ï¼Ÿ

**æ··åˆå¼•æ“ (Hybrid Engine)** å…è®¸åœ¨åŒä¸€ç»„ GPU ä¸Š**å…±äº«**ç”Ÿæˆå¼•æ“ (vLLM/SGLang) å’Œè®­ç»ƒå¼•æ“ (FSDP)ï¼Œé€šè¿‡åŠ¨æ€åˆ‡æ¢æ¨¡å¼å®ç°é«˜æ•ˆåˆ©ç”¨ã€‚

```
åŒä¸€å— GPU:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â”‚
â”‚  æ—¶åˆ» 1: Rollout æ¨¡å¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  vLLM Engine (æ¨ç†)       â”‚     â”‚
â”‚  â”‚  - åŠ è½½ Rollout æƒé‡      â”‚     â”‚
â”‚  â”‚  - ä½¿ç”¨ PagedAttention    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚
â”‚           â†“ åˆ‡æ¢ â†“                 â”‚
â”‚                                    â”‚
â”‚  æ—¶åˆ» 2: Trainer æ¨¡å¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  FSDP Engine (è®­ç»ƒ)       â”‚     â”‚
â”‚  â”‚  - åŠ è½½ Actor å‚æ•°        â”‚     â”‚
â”‚  â”‚  - åŠ è½½ Optimizer         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 æ¨¡å¼åˆ‡æ¢æµç¨‹

```python
class ActorRolloutRefWorker:
    """æ”¯æŒæ··åˆå¼•æ“çš„ Worker"""
    
    def __init__(self, config, role):
        # åˆ¤æ–­æ˜¯å¦å¯ç”¨æ··åˆå¼•æ“
        self.hybrid_engine = config.rollout.hybrid_engine
        
        if self.hybrid_engine:
            self._is_actor = True      # åŒæ—¶å…·æœ‰ Actor èƒ½åŠ›
            self._is_rollout = True    # åŒæ—¶å…·æœ‰ Rollout èƒ½åŠ›
            
            # åˆå§‹åŒ– Actor (FSDP)
            self.actor_module = self._init_actor_model()
            self.actor_optimizer = self._init_optimizer()
            
            # åˆå§‹åŒ– Rollout (vLLM/SGLang)
            self.rollout = self._init_rollout_engine()
        else:
            # ç‹¬ç«‹æ¨¡å¼ï¼šåªæœ‰ä¸€ç§èƒ½åŠ›
            if role == "actor":
                self._is_actor = True
                self._is_rollout = False
            else:
                self._is_actor = False
                self._is_rollout = True
    
    async def rollout_mode(self):
        """åˆ‡æ¢åˆ° Rollout æ¨¡å¼"""
        
        # 1. å¸è½½ Actor å‚æ•°åˆ° CPU
        offload_fsdp_model_to_cpu(self.actor_module)
        offload_fsdp_optimizer(self.actor_optimizer)
        
        # 2. åŒæ­¥æƒé‡åˆ° Rollout å¼•æ“
        weights = self._get_actor_weights()
        await self.rollout.update_weights(weights)
        
        # 3. æ¢å¤ Rollout KV Cache åˆ° GPU
        await self.rollout.resume(tags=["weights", "kv_cache"])
        
        # 4. æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        
        log_gpu_memory_usage("After switch to rollout mode")
    
    async def trainer_mode(self):
        """åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼"""
        
        # 1. é‡Šæ”¾ Rollout èµ„æº
        await self.rollout.release()
        
        # 2. åŠ è½½ Actor å‚æ•°åˆ° GPU
        load_fsdp_model_to_gpu(self.actor_module)
        load_fsdp_optimizer(self.actor_optimizer)
        
        # 3. æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        
        log_gpu_memory_usage("After switch to trainer mode")
    
    def generate_sequences(self, prompts):
        """ç”Ÿæˆåºåˆ— (å¸¦æ¨¡å¼åˆ‡æ¢)"""
        
        # å¦‚æœæ˜¯æ··åˆå¼•æ“ï¼Œéœ€è¦å…ˆåˆ‡æ¢æ¨¡å¼
        if self._is_actor:
            loop = get_event_loop()
            loop.run_until_complete(self.rollout_mode())
        
        # è°ƒç”¨ Rollout å¼•æ“
        output = self.rollout.generate_sequences(prompts)
        
        # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        if self._is_actor:
            loop.run_until_complete(self.trainer_mode())
        
        return output
    
    def update_actor(self, data):
        """æ›´æ–° Actor (è®­ç»ƒ)"""
        
        # æ­¤æ—¶å·²ç»åœ¨ trainer_modeï¼Œç›´æ¥è®­ç»ƒ
        # å‰å‘ä¼ æ’­
        logits = self.actor_module(
            input_ids=data.batch["input_ids"],
            attention_mask=data.batch["attention_mask"],
        )
        
        # è®¡ç®—æŸå¤±
        loss = self._compute_ppo_loss(logits, data)
        
        # åå‘ä¼ æ’­
        self.actor_optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.actor_module.parameters(),
            max_norm=1.0
        )
        
        # æ›´æ–°å‚æ•°
        self.actor_optimizer.step()
        
        return DataProto(meta_info={"metrics": {"loss": loss.item()}})
```

### 5.3 å†…å­˜ä¼˜åŒ–

```python
# æ··åˆå¼•æ“æ˜¾å­˜åˆ†é…ç¤ºä¾‹ (A100 80GB)

# Rollout æ¨¡å¼:
# - vLLM æ¨¡å‹æƒé‡: 14GB (7B FP16)
# - KV Cache: 40GB
# - ä¸´æ—¶æ¿€æ´»: 5GB
# æ€»è®¡: ~59GB

# Trainer æ¨¡å¼:
# - FSDP æ¨¡å‹å‚æ•°: 7GB (Sharded)
# - FSDP æ¢¯åº¦: 7GB
# - Optimizer States: 28GB (AdamW)
# - æ¿€æ´»å€¼: 10GB
# æ€»è®¡: ~52GB

# é€šè¿‡å¸è½½ä¼˜åŒ–:
actor.fsdp_config.param_offload: true       # å‚æ•°å¸è½½åˆ° CPU
actor.fsdp_config.optimizer_offload: true   # Optimizer å¸è½½åˆ° CPU

# Trainer æ¨¡å¼ä¼˜åŒ–å:
# - FSDP å‚æ•° (GPU): 2GB (éƒ¨åˆ†)
# - FSDP å‚æ•° (CPU): 5GB
# - Optimizer (CPU): 28GB
# - æ¿€æ´»å€¼: 10GB
# æ€»è®¡ (GPU): ~12GB
```

---

## 6. å¤šè½®å¯¹è¯ä¸å·¥å…·è°ƒç”¨

### 6.1 é…ç½®

```yaml
actor_rollout_ref:
  rollout:
    multi_turn:
      enable: true
      max_user_turns: 16
      max_assistant_turns: 16
      tool_config_path: recipe/retool/sandbox_fusion_tool_config.yaml
      format: hermes  # æˆ– openai
```

### 6.2 å·¥å…·é…ç½®ç¤ºä¾‹

```yaml
# sandbox_fusion_tool_config.yaml
tools:
  - tool_schema:
      type: function
      function:
        name: code_interpreter
        description: Execute Python code in a sandbox environment
        parameters:
          type: object
          properties:
            code:
              type: string
              description: The Python code to execute
          required:
            - code
    
    tool_implementation:
      class_path: verl.workers.rollout.tools.sandbox
      class_name: SandboxCodeInterpreter
      config:
        timeout: 30
        max_memory: 512  # MB
```

### 6.3 å¤šè½®å¯¹è¯ç¤ºä¾‹

```python
# è¾“å…¥ Prompt:
messages = [
    {
        "role": "user",
        "content": "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬10é¡¹"
    }
]

# ===== Turn 1: LLM ç”Ÿæˆ =====
# Assistant è¾“å‡º:
{
    "role": "assistant",
    "content": "æˆ‘å°†ä½¿ç”¨ Python ä»£ç æ¥è®¡ç®—",
    "tool_calls": [
        {
            "type": "function",
            "function": {
                "name": "code_interpreter",
                "arguments": {
                    "code": "def fib(n):\n    if n <= 1:\n        return n\n    return fib(n-1) + fib(n-2)\n\nresult = fib(10)\nprint(result)"
                }
            }
        }
    ]
}

# ç”Ÿæˆçš„ tokens:
response_ids = [123, 456, 789, ...]  # "æˆ‘å°†ä½¿ç”¨ Python..."
response_mask = [1, 1, 1, ...]       # å…¨ä¸º 1 (LLM ç”Ÿæˆ)

# ===== Turn 2: å·¥å…·æ‰§è¡Œ =====
# æ‰§è¡Œä»£ç ï¼Œè·å¾—ç»“æœ
tool_result = {
    "stdout": "55\n",
    "stderr": "",
    "exit_code": 0
}

# å·¥å…·è¿”å›çš„ tokens:
tool_result_text = json.dumps(tool_result)
tool_ids = tokenizer.encode(tool_result_text)

response_ids.extend(tool_ids)
response_mask.extend([0] * len(tool_ids))  # å·¥å…·è¾“å‡º mask=0

# ===== Turn 3: LLM ç»§ç»­ç”Ÿæˆ =====
# æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
messages.append({
    "role": "tool",
    "name": "code_interpreter",
    "content": tool_result_text
})

# LLM åŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆ
# Assistant è¾“å‡º:
{
    "role": "assistant",
    "content": "æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬10é¡¹æ˜¯ 55"
}

# æœ€ç»ˆçš„ tokens:
final_response_ids = [
    123, 456, 789,      # Turn 1: LLM ç”Ÿæˆ
    234, 567, 890,      # Turn 2: å·¥å…·è¿”å›
    345, 678, 901,      # Turn 3: LLM ç”Ÿæˆ
]

final_response_mask = [
    1, 1, 1,            # Turn 1: mask=1
    0, 0, 0,            # Turn 2: mask=0
    1, 1, 1,            # Turn 3: mask=1
]
```

### 6.4 Response Mask çš„ä½œç”¨

```python
# response_mask ç”¨äºåŒºåˆ†å“ªäº› tokens æ˜¯æ¨¡å‹ç”Ÿæˆçš„ (è®¡ç®—æŸå¤±)

# è®¡ç®— PPO æŸå¤±æ—¶:
policy_loss = compute_policy_loss(
    old_log_prob=old_log_probs,      # (batch_size, response_length)
    log_prob=log_probs,              # (batch_size, response_length)
    advantages=advantages,           # (batch_size, response_length)
    response_mask=response_mask,     # (batch_size, response_length)
)

def compute_policy_loss(old_log_prob, log_prob, advantages, response_mask, ...):
    # åªå¯¹ response_mask=1 çš„ä½ç½®è®¡ç®—æŸå¤±
    ratio = torch.exp(log_prob - old_log_prob)
    
    # Clipped surrogate objective
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 0.8, 1.2) * advantages
    
    policy_loss = -torch.min(surr1, surr2)
    
    # åº”ç”¨ mask: åªè®¡ç®— LLM ç”Ÿæˆçš„ tokens
    policy_loss = policy_loss * response_mask
    
    # å¹³å‡ (åªé™¤ä»¥æœ‰æ•ˆ tokens æ•°é‡)
    policy_loss = policy_loss.sum() / response_mask.sum()
    
    return policy_loss
```

---

## 7. å…·ä½“ä»£ç ç¤ºä¾‹

### 7.1 ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼šå•ä¸ªè®­ç»ƒæ­¥éª¤

```python
# åœ¨ RayPPOTrainer.fit() ä¸­

for batch_dict in train_dataloader:
    # ========== é˜¶æ®µ 1: å‡†å¤‡æ•°æ® ==========
    batch = DataProto.from_single_dict(batch_dict)
    # batch.batch["input_ids"]: (64, 2048)
    # batch.batch["attention_mask"]: (64, 2048)
    
    # æ·»åŠ å”¯ä¸€æ ‡è¯†
    batch.non_tensor_batch["uid"] = np.array([
        str(uuid.uuid4()) for _ in range(64)
    ])
    
    # ========== é˜¶æ®µ 2: Rollout ç”Ÿæˆ ==========
    gen_batch = batch.pop(
        batch_keys=["input_ids", "attention_mask", "position_ids"]
    )
    
    # æ¯ä¸ª prompt ç”Ÿæˆ 16 ä¸ª responses
    gen_batch = gen_batch.repeat(repeat_times=16, interleave=True)
    # ç°åœ¨ batch_size = 64 * 16 = 1024
    
    # è°ƒç”¨ Rollout (å¼‚æ­¥æ¨¡å¼)
    if self.async_rollout_mode:
        gen_output = self.async_rollout_manager.generate_sequences(gen_batch)
    else:
        gen_output = self.actor_rollout_wg.generate_sequences(gen_batch)
    
    # gen_output.batch:
    # - "prompts": (1024, 2048)
    # - "responses": (1024, 2048)
    # - "input_ids": (1024, 4096)
    # - "response_mask": (1024, 2048)
    # - "rollout_log_probs": (1024, 2048)  [å¦‚æœå¯ç”¨]
    
    # ========== é˜¶æ®µ 3: è®¡ç®—å¥–åŠ± ==========
    batch = batch.repeat(repeat_times=16, interleave=True)
    batch = batch.union(gen_output)
    
    # è®¡ç®—å¥–åŠ±åˆ†æ•°
    reward_tensor, reward_extra_info = compute_reward(batch, self.reward_fn)
    batch.batch["token_level_scores"] = reward_tensor
    # token_level_scores: (1024, 2048)
    
    # ========== é˜¶æ®µ 4: è®¡ç®—å¯¹æ•°æ¦‚ç‡ ==========
    old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
    batch = batch.union(old_log_prob)
    # old_log_probs: (1024, 2048)
    
    if self.use_reference_policy:
        ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)
        batch = batch.union(ref_log_prob)
        # ref_log_probs: (1024, 2048)
    
    # ========== é˜¶æ®µ 5: è®¡ç®—ä»·å€¼ ==========
    if self.use_critic:
        values = self.critic_wg.compute_values(batch)
        batch = batch.union(values)
        # values: (1024, 2048)
    
    # ========== é˜¶æ®µ 6: è®¡ç®—ä¼˜åŠ¿ ==========
    # åº”ç”¨ KL æƒ©ç½š (å¦‚æœå¯ç”¨)
    if self.config.algorithm.use_kl_in_reward:
        kld = (batch.batch["old_log_probs"] - 
               batch.batch["ref_log_probs"])
        batch.batch["token_level_rewards"] = (
            batch.batch["token_level_scores"] - 0.01 * kld
        )
    else:
        batch.batch["token_level_rewards"] = batch.batch["token_level_scores"]
    
    # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    batch = compute_advantage(
        batch,
        adv_estimator=AdvantageEstimator.GRPO,
        gamma=1.0,
        lam=0.95,
    )
    # advantages: (1024, 2048)
    # returns: (1024, 2048)
    
    # ========== é˜¶æ®µ 7: æ›´æ–°æ¨¡å‹ ==========
    # æ›´æ–° Critic
    if self.use_critic:
        critic_output = self.critic_wg.update_critic(batch)
    
    # æ›´æ–° Actor
    actor_output = self.actor_rollout_wg.update_actor(batch)
```

### 7.2 è‡ªå®šä¹‰ Rollout å¼•æ“

```python
from verl.workers.rollout.base import BaseRollout
from verl import DataProto

class CustomRollout(BaseRollout):
    """è‡ªå®šä¹‰ Rollout å¼•æ“"""
    
    def __init__(self, config, model_config, device_mesh):
        super().__init__(config, model_config, device_mesh)
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰å¼•æ“
        self.engine = self._init_custom_engine()
    
    def _init_custom_engine(self):
        # å®ç°è‡ªå®šä¹‰åˆå§‹åŒ–
        pass
    
    def generate_sequences(self, prompts: DataProto) -> DataProto:
        """å®ç°ç”Ÿæˆé€»è¾‘"""
        
        # 1. æå–è¾“å…¥
        input_ids = prompts.batch["input_ids"]
        attention_mask = prompts.batch["attention_mask"]
        
        # 2. è°ƒç”¨å¼•æ“ç”Ÿæˆ
        outputs = self.engine.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=self.config.response_length,
            temperature=self.config.temperature,
        )
        
        # 3. æ„å»ºè¾“å‡º
        responses = outputs["sequences"]
        
        # 4. è¿”å› DataProto
        batch = {
            "prompts": input_ids,
            "responses": responses,
            "input_ids": torch.cat([input_ids, responses], dim=-1),
            # ... å…¶ä»–å­—æ®µ
        }
        
        return DataProto(batch=batch)
    
    async def update_weights(self, weights):
        """æ›´æ–°æƒé‡ (æ··åˆå¼•æ“éœ€è¦)"""
        for name, param in weights:
            self.engine.load_weight(name, param)
    
    async def release(self):
        """é‡Šæ”¾èµ„æº (æ··åˆå¼•æ“éœ€è¦)"""
        self.engine.clear_kv_cache()
        torch.cuda.empty_cache()
    
    async def resume(self, tags):
        """æ¢å¤èµ„æº (æ··åˆå¼•æ“éœ€è¦)"""
        if "kv_cache" in tags:
            self.engine.restore_kv_cache()

# æ³¨å†Œè‡ªå®šä¹‰ Rollout
from verl.workers.rollout.base import _ROLLOUT_REGISTRY

_ROLLOUT_REGISTRY[("custom", "sync")] = CustomRollout
```

### 7.3 è‡ªå®šä¹‰ AgentLoop

```python
from verl.experimental.agent_loop.agent_loop import AgentLoopBase

class CustomAgentLoop(AgentLoopBase):
    """è‡ªå®šä¹‰ Agent å¾ªç¯"""
    
    async def run(self, sample):
        """æ‰§è¡Œè‡ªå®šä¹‰é€»è¾‘"""
        
        # 1. åˆå§‹åŒ–
        request_id = str(uuid.uuid4())
        messages = sample["messages"]
        
        # 2. å¤šè½®å¯¹è¯
        for turn in range(self.config.max_turns):
            # ç”Ÿæˆ
            output = await self.server_manager.generate(
                request_id=request_id,
                prompt_ids=self._prepare_prompt(messages),
                sampling_params=self._get_sampling_params(),
            )
            
            # è§£æç»“æœ
            generated_text = self.tokenizer.decode(output.token_ids)
            
            # è‡ªå®šä¹‰é€»è¾‘ï¼šæ£€æµ‹ç‰¹æ®Šæ ‡è®°
            if "<END>" in generated_text:
                break
            
            # å·¥å…·è°ƒç”¨
            if self._has_tool_call(generated_text):
                tool_result = await self._execute_tool(generated_text)
                messages.append({
                    "role": "tool",
                    "content": tool_result
                })
            else:
                break
        
        # 3. è¿”å›ç»“æœ
        return self._build_output(messages)
```

---

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 vLLM ä¼˜åŒ–å‚æ•°

```yaml
actor_rollout_ref:
  rollout:
    name: vllm
    
    # TP å¹¶è¡Œ
    tensor_model_parallel_size: 4
    
    # æ˜¾å­˜åˆ©ç”¨ç‡
    gpu_memory_utilization: 0.9
    
    # KV Cache é…ç½®
    max_num_seqs: 256           # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_num_batched_tokens: 8192  # æœ€å¤§ batch tokens
    
    # æ€§èƒ½ä¼˜åŒ–
    enable_prefix_caching: true  # å‰ç¼€ç¼“å­˜
    disable_log_stats: false     # å¯ç”¨ç»Ÿè®¡æ—¥å¿—
    
    # é‡åŒ–
    quantization: null  # awq, gptq, fp8
```

### 8.2 å¼‚æ­¥æ¨¡å¼ä¼˜åŒ–

```yaml
actor_rollout_ref:
  rollout:
    mode: async
    
    agent:
      num_workers: 8               # å¹¶å‘ worker æ•°
      max_concurrent_requests: 256  # æœ€å¤§å¹¶å‘è¯·æ±‚
      
    # Server æ•°é‡ (è´Ÿè½½å‡è¡¡)
    num_servers: 4
```

### 8.3 Batch Balancing

```python
# åœ¨ RayPPOTrainer ä¸­
def _balance_batch(self, batch, metrics):
    """å¹³è¡¡å„ DP rank çš„è®¡ç®—è´Ÿè½½"""
    
    # 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆ token æ•°
    attention_mask = batch.batch["attention_mask"]
    seqlen_lst = attention_mask.sum(-1).tolist()  # [batch_size]
    
    # 2. å°†æ ·æœ¬åˆ†é…åˆ°å„ DP rank (å¹³è¡¡è´Ÿè½½)
    world_size = self.actor_rollout_wg.world_size
    partitions = get_seqlen_balanced_partitions(
        seqlen_lst,
        k_partitions=world_size,
        equal_size=True,
    )
    
    # 3. é‡æ’åº batch
    global_idx = torch.tensor([
        j for partition in partitions for j in partition
    ])
    batch.reorder(global_idx)
    
    # 4. è®°å½•è´Ÿè½½ä¸å¹³è¡¡åº¦
    unbalance_stats = log_seqlen_unbalance(
        seqlen_list=seqlen_lst,
        partitions=partitions,
    )
    metrics.update(unbalance_stats)

# æ•ˆæœ:
# æœªä¼˜åŒ–: Rank 0: 50000 tokens, Rank 1: 30000 tokens (ä¸å¹³è¡¡)
# ä¼˜åŒ–å: Rank 0: 40000 tokens, Rank 1: 40000 tokens (å¹³è¡¡)
```

---

## æ€»ç»“

### Rollout å…³é”®è¦ç‚¹

1. **èŒè´£**: ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆåºåˆ—
2. **æ¨¡å¼**: 
   - **Sync**: æ‰¹é‡åŒæ­¥ç”Ÿæˆï¼Œç®€å•ç›´æ¥
   - **Async**: å¹¶å‘å¼‚æ­¥ç”Ÿæˆï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨
3. **å¼•æ“**: vLLMã€SGLangã€HuggingFace
4. **æ··åˆå¼•æ“**: ç”Ÿæˆå’Œè®­ç»ƒå…±äº« GPUï¼ŒåŠ¨æ€åˆ‡æ¢æ¨¡å¼
5. **ä¼˜åŒ–**: TP å¹¶è¡Œã€å‰ç¼€ç¼“å­˜ã€è´Ÿè½½å‡è¡¡ã€å¼‚æ­¥å¹¶å‘

### ä½¿ç”¨å»ºè®®

| åœºæ™¯ | æ¨èé…ç½® |
|------|---------|
| **ç®€å•æ–‡æœ¬ç”Ÿæˆ** | vLLM + Sync |
| **å¤šè½®å¯¹è¯** | SGLang + Async |
| **å·¥å…·è°ƒç”¨** | SGLang + Async + Multi-turn |
| **å¤§æ¨¡å‹ (>32B)** | vLLM + TP=8 |
| **æ˜¾å­˜å—é™** | Hybrid Engine + Offload |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-01-06

